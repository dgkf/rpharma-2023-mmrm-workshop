[
  {
    "objectID": "index.html#what-we-will-cover-today",
    "href": "index.html#what-we-will-cover-today",
    "title": "From the statistical method to the R package - the mmrm example",
    "section": "What we will cover today",
    "text": "What we will cover today\n\nShow practical steps for obtaining an R package implementation of a statistical method\n\nDiscuss key considerations for writing statistical software\nIllustrate throughout with the mmrm package development example\n\nShort hands-on introduction to the mmrm package\n\nWorking together across companies and in open source"
  },
  {
    "objectID": "index.html#why-does-this-matter-in-pharma",
    "href": "index.html#why-does-this-matter-in-pharma",
    "title": "From the statistical method to the R package - the mmrm example",
    "section": "Why does this matter in Pharma?",
    "text": "Why does this matter in Pharma?\n“The credibility of the numerical results of the analysis depends on the quality and validity of the methods and software (both internally and externally written) used both for data management […] and also for processing the data statistically. […] The computer software used for data management and statistical analysis should be reliable, and documentation of appropriate software testing procedures should be available.”\n[ICH Topic E 9: Statistical Principles for Clinical Trials, Section 5.8: Integrity of Data and Computer Software Validity]"
  },
  {
    "objectID": "index.html#how-can-we-achive-this",
    "href": "index.html#how-can-we-achive-this",
    "title": "From the statistical method to the R package - the mmrm example",
    "section": "How can we achive this?",
    "text": "How can we achive this?\nHow can we implement statistical methods in R such that\n\nthe software is reliable and\nincludes appropriate testing\n\nto ensure\n\nhigh quality and\nvalidity\n\nand ultimately credibility of the statistical analysis results?"
  },
  {
    "objectID": "index.html#take-away-lessons-for-writing-statistical-software",
    "href": "index.html#take-away-lessons-for-writing-statistical-software",
    "title": "From the statistical method to the R package - the mmrm example",
    "section": "Take away lessons for writing statistical software",
    "text": "Take away lessons for writing statistical software\n\nChoose the right methods and understand them.\nSolve the core implementation problem with prototype code.\nSpend enough time on planning the design of the R package.\nAssume that your R package will be evolving for a long time."
  },
  {
    "objectID": "index.html#why-is-this-important",
    "href": "index.html#why-is-this-important",
    "title": "From the statistical method to the R package - the mmrm example",
    "section": "Why is this important?",
    "text": "Why is this important?\n“The credibility of the numerical results of the analysis depends on the quality and validity of the methods and software …”\n\nIf we don’t choose the right method, then the best software implementation of it won’t help the credibility of the statistical analysis!\nWork together with methods experts (internal, external, …)"
  },
  {
    "objectID": "index.html#how-can-we-understand-the-statistical-method",
    "href": "index.html#how-can-we-understand-the-statistical-method",
    "title": "From the statistical method to the R package - the mmrm example",
    "section": "How can we understand the statistical method?",
    "text": "How can we understand the statistical method?\nWe need to understand the method before implementing it!\n\nIt is not sufficient to just copy/paste code from methods experts\nLet the methods experts present a summary of the methods\nRead an overview paper about the methods\nParaphrase and ask lots of clarifying questions\nUnderstand the details by reading the original paper describing the method"
  },
  {
    "objectID": "index.html#example-mmrm",
    "href": "index.html#example-mmrm",
    "title": "From the statistical method to the R package - the mmrm example",
    "section": "Example: mmrm",
    "text": "Example: mmrm\n\nUnderstand the acronym: Mixed Model with Repeated Measures\nUnderstand the method: It is not a mixed model, just a general linear model\n\nRead an overview paper\n\nUnderstand the problem: In R we did not get the correct adjusted degrees of freedom\n\nTry out existing R packages and compare results with proprietary software\nRead paper describing the adjusted degrees of freedom"
  },
  {
    "objectID": "index.html#example-mmrm-fast-forward",
    "href": "index.html#example-mmrm-fast-forward",
    "title": "From the statistical method to the R package - the mmrm example",
    "section": "Example: mmrm (fast forward)",
    "text": "Example: mmrm (fast forward)\n\nInitial implementation with lme4 workaround (see previous R/Pharma presentation)\nWorks only quite ok for small data sets with few time points\nDoes not converge and takes hours on large data sets with many time points\nTherefore needed to look for another solution"
  },
  {
    "objectID": "index.html#what-is-prototype-code",
    "href": "index.html#what-is-prototype-code",
    "title": "From the statistical method to the R package - the mmrm example",
    "section": "What is prototype code?",
    "text": "What is prototype code?\n\nCan come in different forms, but\n\nis not an R package yet,\nnot documented with roxygen2,\nnot unit tested\n\nIt works usually quite well to have an Rmd or qmd document to combine thoughts and code\nTypically an R script from a methods expert that implements the method can be the start for a prototype"
  },
  {
    "objectID": "index.html#when-have-you-solved-the-core-implementation-problem",
    "href": "index.html#when-have-you-solved-the-core-implementation-problem",
    "title": "From the statistical method to the R package - the mmrm example",
    "section": "When have you solved the core implementation problem?",
    "text": "When have you solved the core implementation problem?\n\nYou have R code that allows you to (half-manually) calculate the results with the chosen methods\nDifferent methods options have been elicited from experts, considered and could be used\nYou have evaluated different solution paths (e.g. using package A or package B as a backbone)\nYou feel that you have solved the hardest part of the problem - everything else is clear how to do it\n(If possible) You have compared the numerical results from your R code with other software, and they match up to numerical accuracy (e.g. relative difference of 0.001)"
  },
  {
    "objectID": "index.html#example-mmrm---try-to-use-existing-packages",
    "href": "index.html#example-mmrm---try-to-use-existing-packages",
    "title": "From the statistical method to the R package - the mmrm example",
    "section": "Example: mmrm - try to use existing packages",
    "text": "Example: mmrm - try to use existing packages\n\nThe hardest part: adjusted degrees of freedom calculation\nTried different solutions with existing R packages:\n\nusing package nlme with emmeans (results are too different, calculation is too approximate)\nusing package lme4 with lmerTest (fails on large data sets with many time points)\nusing package glmmTMB (does not have adjusted degrees of freedom)ß"
  },
  {
    "objectID": "index.html#example-mmrm---try-to-extend-existing-package",
    "href": "index.html#example-mmrm---try-to-extend-existing-package",
    "title": "From the statistical method to the R package - the mmrm example",
    "section": "Example: mmrm - try to extend existing package",
    "text": "Example: mmrm - try to extend existing package\nTried to extend glmmTMB to calculate Satterthwaite adjusted degrees of freedom:\n\nGot in touch with glmmTMB authors and Ben Bolker provided great help and assistance\nUnfortunately it did not work out (results were very far off for unstructed covariance)\nUnderstand that glmmTMB always uses a random effects model representation which is not what we want"
  },
  {
    "objectID": "index.html#example-mmrm---try-to-make-a-custom-implementation",
    "href": "index.html#example-mmrm---try-to-make-a-custom-implementation",
    "title": "From the statistical method to the R package - the mmrm example",
    "section": "Example: mmrm - try to make a custom implementation",
    "text": "Example: mmrm - try to make a custom implementation\nIdea was then to use the Template Model Builder (TMB) library directly:\n\nAs the name suggests, TMB is also used by glmmTMB as the backend\nWe can code with C++ the likelihood of the model we exactly need (general linear model without random effects)\nThe magic: TMB allows to automatically differentiate the (log) likelihood (i.e. by compiling the C++ code)\nThe gradient (and Hessian) can then be used from the R side to find the (restricted) maximum likelihood estimates\nWithin a long weekend, got a working prototype that was fast and matched proprietary software results nicely"
  },
  {
    "objectID": "index.html#why-not-jump-into-writing-functions-right-away",
    "href": "index.html#why-not-jump-into-writing-functions-right-away",
    "title": "From the statistical method to the R package - the mmrm example",
    "section": "Why not jump into writing functions right away?",
    "text": "Why not jump into writing functions right away?\n\nNeed to see the “big picture” first to know how each piece should look like\n\nIncluding definition of the scope of the package - what should be included vs. not\nImportant to discuss “big picture” plan with experts and users regarding workflow\n\nWhen writing a function you should do it together with documentation and unit tests\n\nIf you just start somewhere, chances are very high that you will need to change it later"
  },
  {
    "objectID": "index.html#how-to-plan-the-design-of-the-r-package",
    "href": "index.html#how-to-plan-the-design-of-the-r-package",
    "title": "From the statistical method to the R package - the mmrm example",
    "section": "How to plan the design of the R-package?",
    "text": "How to plan the design of the R-package?\n\nStart with blank sheet of paper to draw flow diagram\n\nWhat parts (functions and classes) can represent the problem most naturally?\n\nDraft a bit more details in doc\n\nWhich arguments for functions, which slots for classes? Names?\n\nGo into Rmd design document and draft prototypes for functions and classes\nBreak down design into separate issues (tasks) to implement\n\nMake notes of dependencies and resulting order of implementation"
  },
  {
    "objectID": "index.html#example-mmrm-1",
    "href": "index.html#example-mmrm-1",
    "title": "From the statistical method to the R package - the mmrm example",
    "section": "Example: mmrm",
    "text": "Example: mmrm\n\nHave a single Rmd as initial design document including prototypes\n\n\n\n---\ntitle: \"Design for fitting MMRM\"\nauthor: \"Daniel Sabanes Bove\"\noutput: html_document\neditor_options:\n  chunk_output_type: console\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\n\n# Objective\n\nWe would like to prototype the whole flow of fitting an MMRM in this new package.\nThis will make subsequent issue solutions more efficient.\n\n```{r}\nlibrary(mmrm)\nlibrary(checkmate)\nlibrary(glmmTMB)\n```\n\n# Example\n\nLet's first set up some example.\n\n```{r}\ndat &lt;- fev_data\nvs &lt;- list(\n  response = \"FEV1\",\n  covariates = c(\"RACE\", \"SEX\"),\n  id = \"USUBJID\",\n  arm = \"ARMCD\",\n  visit = \"AVISIT\"\n)\n```\n\n# Prototypes\n\n## `check_vars()` --&gt; `h_labels()` and `assert_data()`\n\nWe try to simplify the function compared to the old code, using external helpers\nand splitting up the function.\n\n```{r}\nh_is_specified &lt;- function(x, vars) {\n  !is.null(vars[[x]])\n}\nh_is_specified_and_in_data &lt;- function(x, vars, data) {\n  h_is_specified(x, vars) && all(vars[[x]] %in% names(data))\n}\nh_check_and_get_label &lt;- function(x, vars, data) {\n  assert_true(h_is_specified_and_in_data(x, vars, data))\n  res &lt;- NULL\n  for (v in vars[[x]]) {\n    label &lt;- attr(data[[v]], \"label\")\n    string &lt;- ifelse(!is.null(label), label, v)\n    res &lt;- c(res, stats::setNames(string, v))\n  }\n  res\n}\nh_get_covariate_parts &lt;- function(covariates) {\n  unique(unlist(strsplit(covariates, split = \"\\\\*|:\")))\n}\n```\n\nLet's quickly try these out:\n\n```{r}\nh_check_and_get_label(\"arm\", vs, dat)\n```\n\nLet's have a separate `h_labels()` function. This is mostly checking\nthe variable specifications on the side, too.\n\n```{r}\nh_labels &lt;- function(vars,\n                     data) {\n  assert_list(vars)\n  assert_data_frame(data)\n\n  labels &lt;- list()\n\n  labels$response &lt;- h_check_and_get_label(\"response\", vars, data)\n  labels$id &lt;- h_check_and_get_label(\"id\", vars, data)\n  labels$visit &lt;- h_check_and_get_label(\"visit\", vars, data)\n  if (h_is_specified(\"arm\", vars)) {\n    h_check_and_get_label(\"arm\", vars, data)\n  }\n  if (h_is_specified(\"covariates\", vars)) {\n    vars$parts &lt;- h_get_covariate_parts(vars$covariates)\n    labels$parts &lt;- h_check_and_get_label(\"parts\", vars, data)\n  }\n\n  return(labels)\n}\n\nh_labels(vs, dat)\n```\n\nNow let's do the check (assertion) function for the data.\nAgain let's brake it down into manageable pieces.\n\n```{r}\nh_assert_one_rec_pt_visit &lt;- function(vars, data) {\n  # Check there is no more than one record per patient and visit.\n  form &lt;- stats::as.formula(paste(\"~\", vars$visit, \"+\", vars$id))\n  grouped_data &lt;- split(data, f = form)\n  n_per_group &lt;- vapply(grouped_data, nrow, integer(1))\n\n  if (any(n_per_group &gt; 1)) {\n    dupl_group &lt;- which(n_per_group &gt; 1)\n    n_dupl &lt;- length(dupl_group)\n    stop(paste(\n      \"There are\", n_dupl, \"subjects with more than one record per visit:\",\n      toString(names(n_dupl))\n    ))\n  }\n}\n\nh_assert_rsp_var &lt;- function(vars, data) {\n  response_values &lt;- data[[vars$response]]\n  assert_numeric(response_values)\n}\n\nh_assert_visit_var &lt;- function(vars, data) {\n  visit_values &lt;- data[[vars$visit]]\n  assert_factor(visit_values)\n}\n\nassert_data &lt;- function(vars, data) {\n  assert_list(vars)\n  assert_data_frame(data)\n\n  # First subset data to observations with complete regressors.\n  regressor_vars &lt;- c(vars$arm, vars$visit, h_get_covariate_parts(vars$covariates))\n  has_complete_regressors &lt;- stats::complete.cases(data[, regressor_vars])\n  data_complete_regressors &lt;- droplevels(data[has_complete_regressors, ])\n\n  h_assert_one_rec_pt_visit(vars, data_complete_regressors)\n  h_assert_rsp_var(vars, data_complete_regressors)\n  h_assert_visit_var(vars, data_complete_regressors)\n\n  # Second only look at complete data.\n  has_complete_response &lt;- stats::complete.cases(data_complete_regressors[, vars$response])\n  data_complete &lt;- droplevels(data_complete_regressors[has_complete_response, ])\n\n  if (h_is_specified(\"arm\", vars)) {\n    assert_factor(data_complete_regressors[[vars$arm]], min.levels = 2L)\n    assert_factor(\n      data_complete[[vars$arm]],\n      levels = levels(data_complete_regressors[[vars$arm]])\n    )\n    assert_true(all(table(data_complete[[vars$arm]]) &gt; 5))\n  } else {\n    assert_data_frame(data_complete, min.rows = 5L)\n  }\n}\n```\n\nNote that in production the arm checking part could be also put into a\nhelper function to make the `assert_data()` function more consistent.\n\nNow let's try this out, too.\n\n```{r}\nassert_data(vs, dat)\n```\n\n\n## `h_build_formula()`\n\nLet's build the formula for the `glmmTMB` fit call. Basically we want something\nlike this:\n\n`AVAL ~ STRATA1 + BMRKR2 + ARMCD + ARMCD + AVISIT + ARMCD * AVISIT + us(0 + AVISIT | USUBJID)`\n\nwhere the `us` part would look different for covariance structures other than\nthis unstructured one.\n\nFor the `cor_struct` argument we keep a bit more higher level syntax than\n`glmmTMB` itself, since e.g. `us` and `cs` could easily be confused by the user.\n\nNote that for now we don't put in the option to have separate covariance matrices\nper group yet, we can do this in a second pass later on (backlog).\n\n```{r}\nh_build_formula &lt;- function(vars,\n                            cor_struct = c(\n                              \"unstructured\",\n                              \"toeplitz\",\n                              \"auto-regressive\",\n                              \"compound-symmetry\"\n                            )) {\n  assert_list(vars)\n  cor_struct &lt;- match.arg(cor_struct)\n\n  covariates_part &lt;- paste(\n    vars$covariates,\n    collapse = \" + \"\n  )\n  arm_visit_part &lt;- if (is.null(vars$arm)) {\n    vars$visit\n  } else {\n    paste0(\n      vars$arm,\n      \"*\",\n      vars$visit\n    )\n  }\n  random_effects_fun &lt;- switch(cor_struct,\n    \"unstructured\" = \"us\",\n    \"toeplitz\" = \"toep\",\n    \"auto-regressive\" = \"ar1\",\n    \"compound-symmetry\" = \"cs\"\n  )\n  random_effects_part &lt;- paste0(\n    random_effects_fun, \"(0 + \", vars$visit, \" | \", vars$id, \")\"\n  )\n  rhs_formula &lt;- paste(\n    arm_visit_part,\n    \"+\",\n    random_effects_part\n  )\n  if (covariates_part != \"\") {\n    rhs_formula &lt;- paste(\n      covariates_part,\n      \"+\",\n      rhs_formula\n    )\n  }\n  stats::as.formula(paste(\n    vars$response,\n    \"~\",\n    rhs_formula\n  ))\n}\n```\n\nLet's try this out:\n\n```{r}\nh_build_formula(vs, \"toeplitz\")\nh_build_formula(vs)\n```\n\n## `h_cov_estimate()`\n\nLet's see if we even need this function.\n\n```{r}\nmod &lt;- glmmTMB(\n  FEV1 ~ ar1(0 + AVISIT | USUBJID),\n  data = dat,\n  dispformula = ~0,\n  REML = TRUE\n)\nvc &lt;- VarCorr(mod)\nvc$cond[[1]]\nclass(mod) &lt;- c(\"mmrm_fit\", \"glmmTMB\")\n```\n\nOK so that is still not super intuitive, so let's better have the function.\nEspecially as we also want to return how many variance parameters\nthere are. For backwards compatibility we also return one ID which had the\nmaximum number of visits. Maybe later we can remove this again.\n\n```{r}\nh_cov_estimate &lt;- function(model) {\n  assert_class(model, \"mmrm_fit\")\n\n  cov_est &lt;- VarCorr(model)$cond[[1L]]\n  theta &lt;- getME(model, \"theta\")\n  id_per_obs &lt;- model$modelInfo$reTrms$cond$flist[[1L]]\n  n_visits &lt;- length(model$modelInfo$reTrms$cond$cnms[[1L]])\n  which_id &lt;- which(table(id_per_obs) == n_visits)[1L]\n\n  structure(\n    cov_est,\n    id = levels(id_per_obs)[which_id],\n    n_parameters = length(theta)\n  )\n}\n\nstr(h_cov_estimate(mod))\n```\n\nHere we also get the standard deviations and the correlation matrix as\nattributes but that seems useful.\n\n## `h_record_all_output()`\n\nThis is direct copy, and then slightly modified, from `rbmi`.\nTherefore we need to include its author (Craig) as contributors in `mmrm`.\n\n```{r}\n#' Capture all Output\n#'\n#' This function silences all warnings, errors & messages and instead returns a list\n#' containing the results (if it didn't error) + the warning and error messages as\n#' character vectors.\n#'\n#' @param expr (`expression`)\\cr to be executed.\n#' @param remove (`list`)\\cr optional list with elements `warnings`, `errors`,\n#'   `messages` which can be character vectors, which will be removed from the\n#'   results if specified.\n#'\n#' @return\n#' A list containing\n#'\n#' - `result`: The object returned by `expr` or `list()` if an error was thrown\n#' - `warnings`: `NULL` or a character vector if warnings were thrown.\n#' - `errors`: `NULL` or a string if an error was thrown.\n#' - `messages`: `NULL` or a character vector if messages were produced.\n#'\n#' @examples\n#' \\dontrun{\n#' h_record_all_output({\n#'   x &lt;- 1\n#'   y &lt;- 2\n#'   warning(\"something went wrong\")\n#'   message(\"O nearly done\")\n#'   x + y\n#' })\n#' }\nh_record_all_output &lt;- function(expr, remove = list()) {\n  # Note: We don't need to and cannot assert `expr` here.\n  assert_list(remove)\n\n  env &lt;- new.env()\n  result &lt;- withCallingHandlers(\n    withRestarts(\n      expr,\n      muffleStop = function() list()\n    ),\n    message = function(m) {\n      msg_without_newline &lt;- gsub(m$message, pattern = \"\\n$\", replacement = \"\")\n      env$message &lt;- c(env$message, msg_without_newline)\n      invokeRestart(\"muffleMessage\")\n    },\n    warning = function(w) {\n      env$warning &lt;- c(env$warning, w$message)\n      invokeRestart(\"muffleWarning\")\n    },\n    error = function(e) {\n      env$error &lt;- c(env$error, e$message)\n      invokeRestart(\"muffleStop\")\n    }\n  )\n  list(\n    result = result,\n    warnings = setdiff(env$warning, remove$warnings),\n    errors = setdiff(env$error, remove$errors),\n    messages = setdiff(env$message, remove$messages)\n  )\n}\n\nh_record_all_output(\n  {\n    x &lt;- 1\n    y &lt;- 2\n    warning(\"something went wrong\")\n    message(\"O nearly done\")\n    message(\"Almost done\")\n    x + y\n  },\n  remove = list(messages = c(\"Almost done\", \"bla\"))\n)\n```\n\n\n## `fit_single_optimizer()`\n\nHere the optimizers are possible multivariate ones for `stats::optim()`, with\nthe default changed to `L-BFGS-B`. Note that we removed the `SANN` option since\nthat needs very long computation times, so does not seem practical.\n\nWe provide the new possibility for starting values.\n\nWhile this function is not the primary user interface, it can be helpful for users.\nTherefore we don't prefix with `h_`.\n\n```{r}\nfit_single_optimizer &lt;- function(formula,\n                                 data,\n                                 start = NULL,\n                                 optimizer = c(\"L-BFGS-B\", \"Nelder-Mead\", \"BFGS\", \"CG\")) {\n  assert_formula(formula)\n  assert_data_frame(data)\n  assert_list(start, null.ok = TRUE)\n  optimizer &lt;- match.arg(optimizer)\n\n  control &lt;- glmmTMB::glmmTMBControl(\n    optimizer = stats::optim,\n    optArgs = list(method = optimizer),\n    parallel = 1L\n  )\n  quiet_fit &lt;- h_record_all_output(\n    glmmTMB::glmmTMB(\n      formula = formula,\n      data = data,\n      dispformula = ~0,\n      REML = TRUE,\n      start = start,\n      control = control\n    ),\n    remove = list(\n      warnings = c(\n        \"OpenMP not supported.\",\n        \"'giveCsparse' has been deprecated; setting 'repr = \\\"T\\\"' for you\"\n      )\n    )\n  )\n  converged &lt;- (length(quiet_fit$warnings) == 0L) &&\n    (length(quiet_fit$errors) == 0L) &&\n    (quiet_fit$result$fit$convergence == 0)\n  structure(\n    quiet_fit$result,\n    errors = quiet_fit$errors,\n    warnings = quiet_fit$warnings,\n    messages = quiet_fit$messages,\n    optimizer = optimizer,\n    converged = converged,\n    class = c(\"mmrm_fit\", class(quiet_fit$result))\n  )\n}\n```\n\nOK let's try this one out:\n\n```{r}\nmod_fit &lt;- fit_single_optimizer(\n  formula = h_build_formula(vs),\n  data = dat\n)\nattr(mod_fit, \"converged\")\n```\n\nLooks good so far!\n\n## `h_summarize_all_fits()`\n\nNote that we don't return the fixed effects as that is not used downstream.\n\n```{r}\nh_summarize_all_fits &lt;- function(all_fits) {\n  assert_list(all_fits)\n\n  warnings &lt;- lapply(all_fits, attr, which = \"warnings\")\n  messages &lt;- lapply(all_fits, attr, which = \"messages\")\n  log_liks &lt;- vapply(all_fits, stats::logLik, numeric(1L))\n  converged &lt;- vapply(all_fits, attr, logical(1), which = \"converged\")\n\n  list(\n    warnings = warnings,\n    messages = messages,\n    log_liks = log_liks,\n    converged = converged\n  )\n}\n\nh_summarize_all_fits(list(mod_fit, mod_fit))\n```\n\n## `h_free_cores()`\n\nThis is from the `tern.mmrm` package. Since Daniel wrote this function and\nwe will take it out of `tern.mmrm` before publishing no further author\nimplications.\n\nNote that we will need to add the `parallel` and `utils` packages to `Imports`.\n\n```{r}\n#' Get an approximate number of free cores.\n#'\n#' @return the approximate number of free cores, which is an integer between 1 and one less than\n#'   the total cores.\n#'\n#' @details This uses the maximum load average at 1, 5 and 15 minutes on Linux and Mac\n#'   machines to approximate the number of busy cores. For Windows, the load percentage is\n#'   multiplied with the total number of cores.\n#'   We then subtract this from the number of all detected cores. One additional core\n#'   is not used for extra safety.\n#'\n#' @noRd\nh_free_cores &lt;- function() {\n  all_cores &lt;- parallel::detectCores(all.tests = TRUE)\n  busy_cores &lt;-\n    if (.Platform$OS.type == \"windows\") {\n      load_percent_string &lt;- system(\"wmic cpu get loadpercentage\", intern = TRUE)\n      # This gives e.g.: c(\"LoadPercentage\", \"10\", \"\")\n      # So we just take the number here.\n      load_percent &lt;- as.integer(min(load_percent_string[2L], 100))\n      assert_int(load_percent, lower = 0, upper = 100)\n      ceiling(all_cores * load_percent / 100)\n    } else if (.Platform$OS.type == \"unix\") {\n      uptime_string &lt;- system(\"uptime\", intern = TRUE)\n      # This gives e.g.:\n      # \"11:00  up  1:57, 3 users, load averages: 2.71 2.64 2.62\"\n      # Here we just want the last three numbers.\n      uptime_split &lt;- strsplit(uptime_string, split = \",|\\\\s\")[[1]] # Split at comma or white space.\n      uptime_split &lt;- uptime_split[uptime_split != \"\"]\n      load_averages &lt;- as.numeric(utils::tail(uptime_split, 3))\n      ceiling(max(load_averages))\n    }\n  assert_number(all_cores, lower = 1, finite = TRUE)\n  assert_number(busy_cores, lower = 0, upper = all_cores)\n\n  # For safety, we subtract 1 more core from all cores.\n  as.integer(max(1, all_cores - busy_cores - 1))\n}\n\nh_free_cores()\n```\n\nRight now e.g. I have total 16 cores, and I get 14 returned by this function\nwhich makes sense (1 is busy, and 1 is extra buffer).\n\n## `refit_multiple_optimizers()`\n\n```{r}\nrefit_multiple_optimizers &lt;- function(fit,\n                                      n_cores = 1L,\n                                      optimizers = c(\"L-BFGS-B\", \"Nelder-Mead\", \"BFGS\", \"CG\")) {\n  assert_class(fit, \"mmrm_fit\")\n  assert_int(n_cores, lower = 1L)\n  optimizers &lt;- match.arg(optimizers, several.ok = TRUE)\n\n  # Extract the components of the original fit.\n  old_formula &lt;- stats::formula(fit)\n  old_data &lt;- fit$frame\n  old_optimizer &lt;- attr(fit, \"optimizer\")\n\n  # Settings for the new fits.\n  optimizers &lt;- setdiff(optimizers, old_optimizer)\n  n_cores_used &lt;- ifelse(\n    .Platform$OS.type == \"windows\",\n    1L,\n    min(\n      length(optimizers),\n      n_cores\n    )\n  )\n\n  all_fits &lt;- parallel::mclapply(\n    X = optimizers,\n    FUN = fit_single_optimizer,\n    formula = old_formula,\n    data = old_data,\n    start = list(theta = fit$fit$par), # Take the results from old fit as starting values.\n    mc.cores = n_cores_used,\n    mc.silent = TRUE\n  )\n  names(all_fits) &lt;- optimizers\n  all_fits_summary &lt;- h_summarize_all_fits(all_fits)\n\n  # Find the results that are ok:\n  is_ok &lt;- all_fits_summary$converged\n  if (!any(is_ok)) {\n    stop(\n      \"No optimizer led to a successful model fit. \",\n      \"Please try to use a different covariance structure or other covariates.\"\n    )\n  }\n\n  # Return the best result in terms of log-likelihood.\n  best_optimizer &lt;- names(which.max(all_fits_summary$log_liks[is_ok]))\n  best_fit &lt;- all_fits[[best_optimizer]]\n  return(best_fit)\n}\n```\n\nOK, let's try this out. Say we don't converge with the first optimizer choice,\nand then want to run multiple ones.\n\n```{r}\nmod_fit &lt;- fit_single_optimizer(\n  formula = h_build_formula(vs),\n  data = dat,\n  optimizer = \"Nelder-Mead\"\n)\nattr(mod_fit, \"converged\")\nattr(mod_fit, \"warnings\")\n```\n\nSo Nelder-Mead does not converge, and we see a non-positive-definite Hessian\nwarning.\nNow we put this into the refit function:\n\n```{r}\nmod_refit &lt;- refit_multiple_optimizers(mod_fit)\n```\n\n## `fit_model()`\n\nThis is wrapping the lower level fitting functions (single and multiple optimizers).\n\n```{r}\nfit_model &lt;- function(formula,\n                      data,\n                      optimizer = \"automatic\",\n                      n_cores = 1L) {\n  assert_string(optimizer)\n  use_automatic &lt;- identical(optimizer, \"automatic\")\n\n  fit &lt;- fit_single_optimizer(\n    formula = formula,\n    data = data,\n    optimizer = ifelse(use_automatic, \"L-BFGS-B\", optimizer)\n  )\n\n  if (attr(fit, \"converged\")) {\n    fit\n  } else if (use_automatic) {\n    refit_multiple_optimizers(fit, n_cores = n_cores)\n  } else {\n    all_problems &lt;- unlist(\n      attributes(fit)[c(\"errors\", \"messages\", \"warnings\")],\n      use.names = FALSE\n    )\n    stop(paste0(\n      \"Chosen optimizer '\", optimizer, \"' led to problems during model fit:\\n\",\n      paste(paste0(seq_along(all_problems), \") \", all_problems), collapse = \";\\n\"), \"\\n\",\n      \"Consider using the 'automatic' optimizer.\"\n    ))\n  }\n}\n```\n\nLet's try this out quickly too:\n\n```{r}\ntestthat::expect_error(fit_model(\n  formula = h_build_formula(vs),\n  data = dat,\n  optimizer = \"Nelder-Mead\"\n))\n```\n\nSo this gives the expected error message.\n\n```{r}\nmod_fit2 &lt;- fit_model(\n  formula = h_build_formula(vs),\n  data = dat,\n  optimizer = \"BFGS\"\n)\n```\n\nAnd this works.\n\n## `vars()`\n\nJust a little user interface list generator for the variables to use in `mmrm()`.\n\n```{r}\nvars &lt;- function(response = \"AVAL\",\n                 covariates = c(),\n                 id = \"USUBJID\",\n                 arm = \"ARM\",\n                 visit = \"AVISIT\") {\n  list(\n    response = response,\n    covariates = covariates,\n    id = id,\n    arm = arm,\n    visit = visit\n  )\n}\n\nvars()\n```\n\n## `h_vcov_theta()`\n\nThis helper function returns the covariance estimate for the variance parameters\n(`theta`) of the fitted MMRM.\n\n```{r}\nh_vcov_theta &lt;- function(model) {\n  assert_class(model, \"mmrm_fit\")\n\n  model_vcov &lt;- vcov(model, full = TRUE)\n  theta &lt;- getME(model, \"theta\")\n  index_theta &lt;- seq(to = nrow(model_vcov), length = length(theta))\n\n  unname(model_vcov[index_theta, index_theta, drop = FALSE])\n}\n\nvcov_theta &lt;- h_vcov_theta(mod_refit)\n```\n\nHowever one question is now which `theta` parametrization is used here\nto provide the covariance matrix of. Because it is strange that these two\nare not the same:\n\n```{r}\nmod_refit$obj$par\nmod_refit$fit$par\n```\n\nIt could be that the first one are the starting values for the optimization.\nIndeed:\n\n```{r}\nidentical(mod_fit$fit$par, mod_refit$obj$par)\n```\n\n## `h_num_vcov_theta()`\n\nLet's see anyway if we can recover this covariance matrix also numerically:\n\n```{r}\nh_num_vcov_theta &lt;- function(model) {\n  assert_class(model, \"mmrm_fit\")\n\n  theta_est &lt;- model$fit$par\n  devfun_theta &lt;- model$obj$fn\n  hess_theta &lt;- numDeriv::hessian(func = devfun_theta, x = theta_est)\n  eig_hess_theta &lt;- eigen(hess_theta, symmetric = TRUE)\n  with(eig_hess_theta, vectors %*% diag(1 / values) %*% t(vectors))\n}\n\nnum_vcov_theta &lt;- h_num_vcov_theta(mod_refit)\nall.equal(vcov_theta, num_vcov_theta)\nrange(vcov_theta / num_vcov_theta)\n```\n\nSo the results are quite different.\n\n## `h_covbeta_fun()`\n\nFor below we need to construct a function `covbeta_fun` calculating\nthe covariance matrix for the fixed effects (`beta`) as a function of the\nvariance parameters.\n\n```{r}\nh_covbeta_fun &lt;- function(model) {\n  assert_class(model, \"mmrm_fit\")\n\n  function(theta) {\n    sdr &lt;- TMB::sdreport(\n      model$obj,\n      par.fixed = theta,\n      getJointPrecision = TRUE\n    )\n    q_mat &lt;- sdr$jointPrecision\n    which_fixed &lt;- which(rownames(q_mat) == \"beta\")\n    q_marginal &lt;- glmmTMB:::GMRFmarginal(q_mat, which_fixed)\n    unname(solve(as.matrix(q_marginal)))\n  }\n}\n```\n\nLet's try this out: We can recover the usual variance-covariance matrix of the\nfixed effects when plugging in the estimated variance parameters (up to\nreasonable numeric precision), and we get a different result when using\nother variance parameters.\n\n```{r}\ncovbeta_fun &lt;- h_covbeta_fun(mod_refit)\nmod_theta_est &lt;- getME(mod_refit, \"theta\")\ncovbeta_num &lt;- covbeta_fun(theta = mod_theta_est)\ncovbeta_fit &lt;- unname(vcov(mod_refit)$cond)\nall.equal(covbeta_num, covbeta_fit)\nrange(covbeta_num / covbeta_fit)\nrange(covbeta_num - covbeta_fun(theta = mod_theta_est * 0.9))\n```\n\n## `h_general_jac_list()`\n\nWe also need to compute the jacobian, and organize it as a list.\n\nWe start with a general helper that takes a function `covbeta_fun` (see above),\nas well as `x_opt` which is the variance parameter estimate.\n\n```{r}\nh_general_jac_list &lt;- function(covbeta_fun,\n                               x_opt,\n                               ...) {\n  assert_function(covbeta_fun, nargs = 1L)\n  assert_numeric(x_opt, any.missing = FALSE, min.len = 1L)\n\n  jac_matrix &lt;- numDeriv::jacobian(\n    func = covbeta_fun,\n    x = x_opt,\n    # Note: No need to specify further `methods.args` here.\n    ...\n  )\n  get_column_i_as_matrix &lt;- function(i) {\n    # This column contains the p x p entries.\n    jac_col &lt;- jac_matrix[, i]\n    p &lt;- sqrt(length(jac_col))\n    # Obtain p x p matrix.\n    matrix(jac_col, nrow = p, ncol = p)\n  }\n  lapply(\n    seq_len(ncol(jac_matrix)),\n    FUN = get_column_i_as_matrix\n  )\n}\n```\n\nLet's try this out, too:\n\n```{r}\njac_example &lt;- h_general_jac_list(covbeta_fun, mod_theta_est)\n```\n\nSo this takes a few seconds to generate. However this is still in the same\nballpark as the fitting of the MMRM itself, so should not be practical problem.\n\n## `h_jac_list()`\n\nNow we can wrap this in a function that takes the fitted MMRM directly and\nreturns the Jacobian.\n\n```{r}\nh_jac_list &lt;- function(model) {\n  covbeta_fun &lt;- h_covbeta_fun(model)\n  theta_est &lt;- getME(model, \"theta\")\n  h_general_jac_list(covbeta_fun = covbeta_fun, x_opt = theta_est)\n}\n\njac_list &lt;- h_jac_list(mod_refit)\n```\n\n## `mmrm()`\n\nThis is the primary user interface for performing the MMRM analysis. It returns\nan object of class `mmrm` and further methods are provided then to work with\nthese objects (summary, least square means, etc.)\n\nNote that since the least square means and model diagnostics are downstream\ncalculations, we no longer include them in the object itself.\n\n```{r}\nmmrm &lt;- function(data,\n                 vars = vars(),\n                 conf_level = 0.95,\n                 cor_struct = \"unstructured\",\n                 weights_emmeans = \"proportional\",\n                 optimizer = \"automatic\",\n                 parallel = FALSE) {\n  assert_data(vars, data)\n  assert_number(conf_level, lower = 0, upper = 1)\n  assert_flag(parallel)\n\n  labels &lt;- h_labels(vars, data)\n  formula &lt;- h_build_formula(vars, cor_struct)\n  model &lt;- fit_model(\n    formula = formula,\n    data = data,\n    optimizer = optimizer,\n    n_cores = ifelse(parallel, h_free_cores(), 1L)\n  )\n  vcov_theta &lt;- h_num_vcov_theta(model)\n  jac_list &lt;- h_jac_list(model)\n  vcov_beta &lt;- vcov(model)$cond\n  beta_est &lt;- fixef(model)$cond\n  cov_est &lt;- h_cov_estimate(model)\n  ref_level &lt;- if (is.null(vars$arm)) NULL else levels(data[[vars$arm]])[1]\n\n  results &lt;- list(\n    model = model,\n    vcov_theta = vcov_theta,\n    jac_list = jac_list,\n    vcov_beta = vcov_beta,\n    beta_est = beta_est,\n    cov_est = cov_est,\n    vars = vars,\n    labels = labels,\n    ref_level = ref_level,\n    conf_level = conf_level\n  )\n  class(results) &lt;- \"mmrm\"\n  results\n}\n```\n\nLet's try this out:\n\n```{r}\nresult &lt;- mmrm(dat, vs)\n```\n\nTakes about 13 seconds on my computer here, so that is alright.\n\n## `diagnostics()`\n\nNow we want to compute the model diagnostic statistics. Note that here\nwe start now from the full `mmrm` object which already has the covariance\nestimate, so we don't need to compute this again.\n\nNote: In production this can be a generic function with a method for `mmrm`.\n\n```{r}\ndiagnostics &lt;- function(object) {\n  assert_class(object, \"mmrm\")\n\n  n_obs &lt;- object$model$modelInfo$nobs\n  cov_est &lt;- object$cov_est\n  df &lt;- attr(cov_est, \"n_parameters\")\n  n_fixed &lt;- ncol(getME(object$model, \"X\"))\n  m &lt;- max(df + 2, n_obs - n_fixed)\n  log_lik &lt;- as.numeric(stats::logLik(object$model))\n  n_subjects &lt;- nlevels(object$model$modelInfo$reTrms$cond$flist[[1L]])\n\n  list(\n    \"REML criterion\" = -2 * log_lik,\n    AIC = -2 * log_lik + 2 * df,\n    AICc = -2 * log_lik + 2 * df * (m / (m - df - 1)),\n    BIC = -2 * log_lik + df * log(n_subjects)\n  )\n}\n\ndiagnostics(result)\n```\n\n## `h_quad_form_vec()`\n\nJust a small numeric helper to compute a quadratic form of a vector\nand a matrix.\n\n```{r}\n# Calculates x %*% mat %*% t(x) efficiently if x is a (row) vector.\nh_quad_form_vec &lt;- function(x, mat) {\n  assert_numeric(x, any.missing = FALSE)\n  assert_matrix(mat, mode = \"numeric\", any.missing = FALSE, nrows = length(x), ncols = length(x))\n  sum(x * (mat %*% x))\n}\n\nh_quad_form_vec(1:2, matrix(1:4, 2, 2))\n```\n\n## `h_gradient()`\n\nThis is the helper to compute the gradient based on a jacobian (as a list, as above)\nand a vector `L`.\n\n```{r}\nh_gradient &lt;- function(jac_list, L) {\n  assert_list(jac_list)\n  assert_numeric(L)\n\n  vapply(\n    jac_list,\n    h_quad_form_vec, # = {L' Jac L}_i\n    x = L,\n    numeric(1L)\n  )\n}\n\nL &lt;- c(-1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0)\nh_gradient(jac_example, L)\n```\n\n## `h_df_1d_list()`\n\nLittle helper function to format results of `df_1d()`.\n\n```{r}\nh_df_1d_list &lt;- function(est, se, v_num, v_denom) {\n  t_stat &lt;- est / se\n  df &lt;- v_num / v_denom\n  pval &lt;- 2 * pt(q = abs(t_stat), df = df, lower.tail = FALSE)\n\n  list(\n    est = est,\n    se = se,\n    df = df,\n    t_stat = t_stat,\n    pval = pval\n  )\n}\n```\n\n## `df_1d()`\n\nWe define this function to calculate the Satterthwaite degrees of freedom for the\none-dimensional case. It takes the `mmrm` object and the contrast matrix (here\nvector).\n\n```{r}\ndf_1d &lt;- function(object, L) {\n  assert_class(object, \"mmrm\")\n  assert_numeric(L, any.missing = FALSE)\n\n  L &lt;- as.vector(L)\n  assert_numeric(L, len = length(object$beta_est))\n  contrast_est &lt;- sum(L * object$beta_est)\n  contrast_var &lt;- h_quad_form_vec(L, object$vcov_beta)\n  contrast_grad &lt;- h_gradient(object$jac_list, L)\n\n  v_numerator &lt;- 2 * contrast_var^2\n  v_denominator &lt;- h_quad_form_vec(contrast_grad, object$vcov_theta)\n\n  h_df_1d_list(\n    est = contrast_est,\n    se = sqrt(contrast_var),\n    v_num = v_numerator,\n    v_denom = v_denominator\n  )\n}\n```\n\nLet's try it out:\n\n```{r}\ndf_1d(result, L)\n```\n\nLet's quickly compare this with the existing `tern.mmrm` just to be sure\nthat this is correct:\n\n```{r}\nold_result &lt;- tern.mmrm::fit_mmrm(vars = vs, data = dat)\nall.equal(result$beta_est, fixef(old_result$fit))\nlmerTest::contest1D(old_result$fit, L)\n```\n\nSo unfortunately that the degrees of freedom results are very far apart.\nThe question is why?\n\nLet's look inside:\n\n```{r}\n# debug(lmerTest:::contest1D.lmerModLmerTest)\nlmerTest::contest1D(old_result$fit, L)\n```\n\nThe denominator of the degrees of freedom is 0.0213 so completely different\nthan what we have above with 11.93. The numerator of the degrees of freedom\nis 5.303 which is similar what we have with 5.425.\n\nNow for the denominator the problem is again that the parametrization is\ndifferent between `lme4` and `glmmTMB` so that we cannot compare directly\nthe inputs, i.e. the gradient for the variance of the contrast evaluated\nat the variance parameter estimates, and the covariance matrix for variance\nparameters.\n\nLet's quickly try another path to obtain the gradient numerically.\nWe write directly the contrast variance estimate as a function of `theta`.\n\n```{r}\nh_contrast_var_fun &lt;- function(model, L) {\n  covbeta_fun &lt;- h_covbeta_fun(model)\n  function(theta) {\n    covbeta &lt;- covbeta_fun(theta)\n    h_quad_form_vec(L, covbeta)\n  }\n}\n\ncontrast_var_fun &lt;- h_contrast_var_fun(result$model, L)\nsqrt(contrast_var_fun(mod_theta_est))\n```\n\nOK, this matches what we have above.\nNow we can calculate the gradient again using `numDeriv`:\n\n```{r}\nnum_grad_contrast_var &lt;- numDeriv::grad(contrast_var_fun, mod_theta_est)\n```\n\nIt is interesting that this takes quite a while to compute.\n\nNow we can compare this with what we have via the Jacobian:\n\n```{r}\ncontrast_grad &lt;- h_gradient(result$jac_list, L)\nall.equal(num_grad_contrast_var, contrast_grad)\nnum_grad_contrast_var - contrast_grad\n```\n\nSo this is actually quite different. But even then we would get as\ndenominator:\n\n```{r}\nnum_v_denom &lt;- h_quad_form_vec(num_grad_contrast_var, result$vcov_theta)\nnum_v_denom\n```\n\nwhich is not what we expect. But on the other hand\n\n```{r}\n1 / num_v_denom\n```\n\nis very close to what we would expect? But I don't understand why at all.\n\nThe other problem to debug this now here is that for the `lme4` model we have 11\nvariance parameters (one too much), whereas for `glmmTMB` we only have 10.\nSo we cannot easily transform the variance parameters into each other.\n\nThe problem is that obviously the `glmmTMB` derived degrees of freedom are\nwrong. We can e.g. have another comparison via simplified degrees of freedom:\n\n```{r}\nsimple_df &lt;- length(unique(result$model$frame$USUBJID)) -\n  Matrix::rankMatrix(model.matrix(FEV1 ~ RACE + SEX + ARMCD * AVISIT, data = dat))[1]\nsimple_df\n```\n\nwhich is at least in a similar ballpark.\n\nTemporary conclusion: It seems that `h_covbeta_fun()` is not accurate enough.\nWe will replace this as soon as possible with an improved version. The flow\nof the code and the other functions can stay however. Therefore we proceed\nas planned.\n\n## `h_quad_form_mat()`\n\nJust another helper to compute a quadratic form of a matrix and another matrix.\n\n```{r}\n# Calculates x %*% mat %*% t(x) efficiently if x is a matrix.\nh_quad_form_mat &lt;- function(x, mat) {\n  assert_matrix(x, mode = \"numeric\", any.missing = FALSE)\n  assert_matrix(mat, mode = \"numeric\", any.missing = FALSE, nrows = ncol(x), ncols = ncol(x))\n  x %*% tcrossprod(mat, x)\n}\n\nh_quad_form_mat(\n  x = matrix(1:2, 1, 2),\n  mat = matrix(1:4, 2, 2)\n)\n```\n\n## `h_df_md_list()`\n\nLittle helper function to format results of `df_md()`.\n\n```{r}\nh_df_md_list &lt;- function(f_stat, num_df, denom_df, scale = 1) {\n  f_stat &lt;- f_stat * scale\n  pval &lt;- pf(q = f_stat, df1 = num_df, df2 = denom_df, lower.tail = FALSE)\n\n  list(\n    num_df = num_df,\n    denom_df = denom_df,\n    f_stat = f_stat,\n    pval = pval\n  )\n}\n```\n\n## `h_md_denom_df()`\n\nThis helper computes the denominator degrees of freedom for the F-statistic,\nwhen derived from squared t-statistics. If the input values are two similar to\neach other then just the average is returned.\n\n```{r}\nh_md_denom_df &lt;- function(t_stat_df) {\n  assert_numeric(t_stat_df, min.len = 1L, lower = .Machine$double.xmin, any.missing = FALSE)\n  if (test_scalar(t_stat_df)) {\n    return(t_stat_df)\n  }\n  if (all(abs(diff(t_stat_df)) &lt; 1e-8)) {\n    return(mean(t_stat_df))\n  }\n  if (any(t_stat_df &lt;= 2)) {\n    2\n  } else {\n    E &lt;- sum(t_stat_df / (t_stat_df - 2))\n    2 * E / (E - (length(t_stat_df)))\n  }\n}\n\nh_md_denom_df(1:5)\nh_md_denom_df(c(2.5, 4.6, 2.3))\n```\n\n## `df_md()`\n\nWe define this function to calculate the Satterthwaite degrees of freedom for\nthe multi-dimensional case. It takes the `mmrm` object and the contrast matrix\n(here vector).\n\n```{r}\ndf_md &lt;- function(object, L) {\n  assert_class(object, \"mmrm\")\n  assert_numeric(L, any.missing = FALSE)\n\n  if (!is.matrix(L)) {\n    L &lt;- matrix(L, ncol = length(L))\n  }\n  assert_matrix(L, ncol = length(object$beta_est))\n\n  # Early return if we are in the one-dimensional case.\n  if (identical(nrow(L), 1L)) {\n    res_1d &lt;- df_1d(object, L)\n    return(h_df_md_list(f_stat = res_1d$t_stat^2, num_df = 1, denom_df = res_1d$df))\n  }\n\n  contrast_cov &lt;- h_quad_form_mat(L, object$vcov_beta)\n  eigen_cont_cov &lt;- eigen(contrast_cov)\n  eigen_cont_cov_vctrs &lt;- eigen_cont_cov$vectors\n  eigen_cont_cov_vals &lt;- eigen_cont_cov$values\n\n  eps &lt;- sqrt(.Machine$double.eps)\n  tol &lt;- max(eps * eigen_cont_cov_vals[1], 0)\n  rank_cont_cov &lt;- sum(eigen_cont_cov_vals &gt; tol)\n  assert_number(rank_cont_cov, lower = .Machine$double.xmin)\n  rank_seq &lt;- seq_len(rank_cont_cov)\n  vctrs_cont_prod &lt;- crossprod(eigen_cont_cov_vctrs, L)[rank_seq, , drop = FALSE]\n\n  # Early return if rank 1.\n  if (identical(rank_cont_cov, 1L)) {\n    res_1d &lt;- df_1d(object, L)\n    return(h_df_md_list(f_stat = res_1d$t_stat^2, num_df = 1, denom_df = res_1d$df))\n  }\n\n  t_squared_nums &lt;- drop(vctrs_cont_prod %*% result$beta_est)^2\n  t_squared_denoms &lt;- eigen_cont_cov_vals[rank_seq]\n  t_squared &lt;- t_squared_nums / t_squared_denoms\n  f_stat &lt;- sum(t_squared) / rank_cont_cov\n  grads_vctrs_cont_prod &lt;- lapply(rank_seq, \\(m) h_gradient(result$jac_list, L = vctrs_cont_prod[m, ]))\n  t_stat_df_nums &lt;- 2 * eigen_cont_cov_vals^2\n  t_stat_df_denoms &lt;- vapply(grads_vctrs_cont_prod, h_quad_form_vec, mat = object$vcov_theta, numeric(1))\n  t_stat_df &lt;- t_stat_df_nums / t_stat_df_denoms\n  denom_df &lt;- h_md_denom_df(t_stat_df)\n\n  h_df_md_list(\n    f_stat = f_stat,\n    num_df = rank_cont_cov,\n    denom_df = denom_df\n  )\n}\n\nL2 &lt;- rbind(\n  c(-1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n  c(0, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0)\n)\ndf_md(result, L2)\ndf_md(result, L)\n```\n\n## `recover_data` method\n\nHere the idea is that we just forward directly to the `glmmTMB` method so\nthat we don't have to do anything ourselves.\n\n```{r}\nlibrary(emmeans)\nrecover_data.mmrm &lt;- function(object, ...) {\n  component &lt;- \"cond\"\n  emmeans::recover_data(object$model, component = \"cond\", ...)\n}\n```\n\nLet's try if this does something.\n\n```{r}\ntest &lt;- recover_data(result)\nclass(test)\ndim(test)\n```\n\nSo that seems to work fine.\n\n## `emm_basis` method\n\nAlso here the majority of the work can be done by the `glmmTMB` method.\nWe just need to replace the `dffun` and `dfargs` in the list that is returned\nby that before returning ourselves. For this we look at the `merMod` method in\n`emmeans` (https://github.com/rvlenth/emmeans/blob/0af291a78eaecb9e22f45b5ec064474f5f5ed61a/R/helpers.R#L192).\n\n```{r}\nemm_basis.mmrm &lt;- function(object, trms, xlev, grid, vcov., ...) {\n  res &lt;- emm_basis(object$model, trms = trms, xlev = xlev, grid = grid, vcov. = vcov., component = \"cond\", ...)\n  dfargs &lt;- list(object = object)\n  dffun &lt;- function(k, dfargs) {\n    # Note: Once this is `df_md` function is in the package we can just get\n    # it from there instead from global environment.\n    get(\"df_md\", envir = globalenv())(dfargs$object, k)$denom_df\n  }\n  res$dfargs &lt;- dfargs\n  res$dffun &lt;- dffun\n  res\n}\n```\n\nNow let's see if we can use `emmeans` on the `mmrm` object:\n\n```{r}\nemm_obj &lt;- emmeans(result, c(\"ARMCD\", \"AVISIT\"), data = dat)\nemm_obj\n```\n\nSo that gives different result than calling directly the `glmmTMB` method:\n\n```{r}\nemm_obj2 &lt;- emmeans(result$model, c(\"ARMCD\", \"AVISIT\"), data = dat)\nemm_obj2\n```\n\nSo our own degrees of freedom are used successfully!\n\nBased on this we can then calculate any least square means, e.g.:\n\n```{r}\npairs(emm_obj)\n```\n\nNote that we have numerical problems here because of the wrong Satterthwaite\ncalculations at this point. But all this will be fixed when `h_covbeta_fun()`\nis fixed."
  },
  {
    "objectID": "index.html#example-mmrm-contd",
    "href": "index.html#example-mmrm-contd",
    "title": "From the statistical method to the R package - the mmrm example",
    "section": "Example: mmrm (cont’d)",
    "text": "Example: mmrm (cont’d)\n\nHave separate issues and corresponding pull requests implementing functions"
  },
  {
    "objectID": "index.html#why-should-we-document-the-methods",
    "href": "index.html#why-should-we-document-the-methods",
    "title": "From the statistical method to the R package - the mmrm example",
    "section": "Why should we document the methods?",
    "text": "Why should we document the methods?\n\nIt is important to add method documentation in your package, typically as a vignette\nThis provides the “glue” between the original methods paper and your implementation in code\n\ndifferent mathematical symbols compared to original paper but that match the code variable names\nspecific details on the algorithm\n\nUsers benefit from this method documentation a lot because they can understand what is going on in your package\nDevelopers will depend on the method documentation when adding new method features and to understand the code"
  },
  {
    "objectID": "index.html#why-do-we-need-tests",
    "href": "index.html#why-do-we-need-tests",
    "title": "From the statistical method to the R package - the mmrm example",
    "section": "Why do we need tests?",
    "text": "Why do we need tests?\n\nIt is 100% guaranteed that users will have new feature requests after the first version of the R package has been released\n\nThis is a good sign - the package is being used and your users have good ideas!\n\nIt is also very likely that some of the packages you depend on will change - but you want to be sure your package still works\n\nincl. integration tests, making sure numerical results are still correct\n\nSo you will need to change the code …\n\n… but you can only do that comfortably if you know that the package still works afterwards\nIf the tests pass you know it still works!"
  },
  {
    "objectID": "index.html#how-can-i-make-the-package-extensible",
    "href": "index.html#how-can-i-make-the-package-extensible",
    "title": "From the statistical method to the R package - the mmrm example",
    "section": "How can I make the package extensible?",
    "text": "How can I make the package extensible?\n\n“Extensible” = others can extend it without changing package code\nYou want to make your package extensible so that you or other developers can easily extend it\nPrefer to combine multiple functions in pipelines (similar as typically done in tidyverse)\nPrefer object oriented package designs because it will help a lot the extensibility\nGenerally avoid functions with many arguments or longer than 50 lines of code"
  },
  {
    "objectID": "index.html#example-mmrm---method-documentation",
    "href": "index.html#example-mmrm---method-documentation",
    "title": "From the statistical method to the R package - the mmrm example",
    "section": "Example: mmrm - method documentation",
    "text": "Example: mmrm - method documentation\n\nStarted with handwritten notes of the algorithm implementation for the prototype\nTranslated that into vignette\nHas been updated many times already when algorithm was updated\nMeanwhile have in total 12 different vignettes on different aspects"
  },
  {
    "objectID": "index.html#example-mmrm---tests",
    "href": "index.html#example-mmrm---tests",
    "title": "From the statistical method to the R package - the mmrm example",
    "section": "Example: mmrm - tests",
    "text": "Example: mmrm - tests\n\nAdd tests, code documentation and method documentation during each pull request for each function\n\nAdd integration tests comparing numerical results with numbers sourced from proprietary software\nSome tests can take longer, if run time becomes an issue can skip them on CRAN\n\nTurned out tests were super important because minor C++ changes could break results on different operating system"
  },
  {
    "objectID": "index.html#example-mmrm---extensibility",
    "href": "index.html#example-mmrm---extensibility",
    "title": "From the statistical method to the R package - the mmrm example",
    "section": "Example: mmrm - extensibility",
    "text": "Example: mmrm - extensibility\n\nThis is a typical “model fitting” package and therefore we use the S3 class system\n\nOver time can add interfaces to other modeling packages (more later)"
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "From the statistical method to the R package - the mmrm example",
    "section": "Installation",
    "text": "Installation\n\nCRAN as usual: install.packages(\"mmrm\")\n\nLags a bit behind at the moment (due to CRAN manual review bottleneck)\n\nGitHub as usual: remotes::install_github(\"openpharma/mmrm\")\n\nBut needs C++ toolchain and can take quite a while to compile\n\nR-Universe: https://openpharma.r-universe.dev/mmrm and download the binary package and install afterwards\n\nSomehow the install.packages() path from R does not find the binaries"
  },
  {
    "objectID": "index.html#features-of-mmrm-0.3",
    "href": "index.html#features-of-mmrm-0.3",
    "title": "From the statistical method to the R package - the mmrm example",
    "section": "Features of mmrm (>= 0.3)",
    "text": "Features of mmrm (&gt;= 0.3)\n\nLinear model for dependent observations within independent subjects\nCovariance structures for the dependent observations:\n\nUnstructured, Toeplitz, AR1, compound symmetry, ante-dependence, spatial exponential\nAllows group specific covariance estimates and weights\n\nREML or ML estimation, using multiple optimizers if needed\nRobust sandwich estimator for covariance\nDegrees of freedom adjustments: Satterthwaite, Kenward-Roger, Kenward-Roger-Linear, Between-Within, Residual"
  },
  {
    "objectID": "index.html#ecosystem-integration",
    "href": "index.html#ecosystem-integration",
    "title": "From the statistical method to the R package - the mmrm example",
    "section": "Ecosystem integration",
    "text": "Ecosystem integration\n\nemmeans interface for least square means\ntidymodels for easy model fitting:\n\nDedicated parsnip engine for linear regression\nIntegration with recipes\n\nNEST family for clinical trial reporting:\n\ntern.mmrm to easily generate common tables and plot (coming to CRAN soon)\nteal.modules.clinical to easily spin up Shiny app based on the teal framework (coming to CRAN soon)\n\nProvided by third party packages (remember the extensibility discussion):\n\ninterfaces to insight, parameters"
  },
  {
    "objectID": "index.html#unit-and-integration-testing",
    "href": "index.html#unit-and-integration-testing",
    "title": "From the statistical method to the R package - the mmrm example",
    "section": "Unit and integration testing",
    "text": "Unit and integration testing\n\nUnit tests can be found in the GitHub repository under ./tests.\nUses the testthat framework with covr to communicate the testing coverage.\n\nCoverage above 95%\nAlso include tests for key C++ functions\n\nThe integration tests in mmrm are set to a standard tolerance of \\(10^{-3}\\) when compared to other software outputs\n\nComparison with SAS results (PROC MIXED)\nComparison with relevant R packages"
  },
  {
    "objectID": "index.html#benchmarking-with-other-r-packages",
    "href": "index.html#benchmarking-with-other-r-packages",
    "title": "From the statistical method to the R package - the mmrm example",
    "section": "Benchmarking with other R packages",
    "text": "Benchmarking with other R packages\n\nCompared mmrm::mmrm with nlme::gls, lme4::lmer, glmmTMB::glmmTMB\nHighlights:\n\nmmrm has faster convergence time\n\nUsing FEV dataset as an example, mmrm took ~50 ms, while lmer ~200 ms, gls and glmmTMB &gt;600 ms\n\nmmrm and gls estimates have smaller differences from SAS PROC GLIMMIX estimates\nmmrm and gls are more resilient to missingness\n\nDetailed results at the online comparison vignette"
  },
  {
    "objectID": "index.html#impact-of-mmrm",
    "href": "index.html#impact-of-mmrm",
    "title": "From the statistical method to the R package - the mmrm example",
    "section": "Impact of mmrm",
    "text": "Impact of mmrm\n\nCRAN downloads: around 100 per day in Oct 2023\n\nhttps://cran.r-project.org/web/packages/mmrm/\nnew CRAN release v0.3 coming any day now!\n\nGitHub repository: 73 stars as of 17 Oct 2023\n\nhttps://github.com/openpharma/mmrm\n\nPart of CRAN clinical trials task view"
  },
  {
    "objectID": "index.html#outlook",
    "href": "index.html#outlook",
    "title": "From the statistical method to the R package - the mmrm example",
    "section": "Outlook",
    "text": "Outlook\n\nmmrm is now relatively complete for mostly needed features\nWe still have a few major ideas on our backlog:\n\nType II and Type III ANOVA tests\nEvaluate adding (simple) random effects\n\nPlease let us know what is missing in mmrm for you!"
  },
  {
    "objectID": "index.html#demo-instructions",
    "href": "index.html#demo-instructions",
    "title": "From the statistical method to the R package - the mmrm example",
    "section": "Demo Instructions",
    "text": "Demo Instructions\nDoug can you include here basic instructions (maybe screenshot if needed) how folks can log into the Posit cloud for the demo?"
  },
  {
    "objectID": "index.html#introducing-openstatsware",
    "href": "index.html#introducing-openstatsware",
    "title": "From the statistical method to the R package - the mmrm example",
    "section": "Introducing openstatsware",
    "text": "Introducing openstatsware\n\n\nFounded last year:\n\nWhen: 19 August 2022 - just celebrated our 1 year birthday!\nWhere: American Statistical Association (ASA) Biopharmaceutical Section (BIOP)\nInitially: 11 statisticians from 7 pharma companies developing statistical software\nNew name: openstatsware"
  },
  {
    "objectID": "index.html#mmrm-was-our-first-workstream",
    "href": "index.html#mmrm-was-our-first-workstream",
    "title": "From the statistical method to the R package - the mmrm example",
    "section": "mmrm was our first workstream",
    "text": "mmrm was our first workstream\n\nWhy is the MMRM topic important?\n\nMMRM is a popular analysis method for longitudinal continuous outcomes in randomized clinical trials\nAlso used as backbone for more recent methods such as multiple imputation\n\nSee also our second workstream that produced brms.mmrm\n\nBayesian inference in MMRM, based on brms (as Stan frontend for HMC sampling)"
  },
  {
    "objectID": "index.html#human-success-factors",
    "href": "index.html#human-success-factors",
    "title": "From the statistical method to the R package - the mmrm example",
    "section": "Human success factors",
    "text": "Human success factors\n\nMutual interest and mutual trust\nPrerequisite is getting to know each other\n\nAlthough mostly just online, biweekly calls help a lot with this\n\nReciprocity mindset\n\n“Reciprocity means that in response to friendly actions, people are frequently much nicer and much more cooperative than predicted by the self-interest model”\nPersonal experience: If you first give away something, more will come back to you."
  },
  {
    "objectID": "index.html#be-inclusive-in-the-development",
    "href": "index.html#be-inclusive-in-the-development",
    "title": "From the statistical method to the R package - the mmrm example",
    "section": "Be inclusive in the development",
    "text": "Be inclusive in the development\n\nImportant to go public as soon as possible\n\nWe did not wait for mmrm to be finished before initial open sourcing\nMany developers contributed over time\n\nBuilding software together works better than alone\n\nDifferent perspectives in discussions and code review help to optimize the user interface and thus experience\nBe generous with authorship"
  },
  {
    "objectID": "index.html#practical-daily-development-process",
    "href": "index.html#practical-daily-development-process",
    "title": "From the statistical method to the R package - the mmrm example",
    "section": "Practical daily development process",
    "text": "Practical daily development process\nDoug would you like to add here a few slides?\n\n\nFrom the statistical method to the R package - the mmrm example | License"
  }
]