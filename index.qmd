---
title: "From the statistical method to the R package - the `mmrm` example"
author: "Daniel SabanÃ©s BovÃ©, Doug Kelkhoff (Roche)"
institute: "R/Pharma Workshop"
date: "October 19 2023"
date-format: long
format: 
  revealjs:
    fontsize: 2rem
    slide-number: true
    footer: 'From the statistical method to the R package - the `mmrm` example | [License](http://creativecommons.org/licenses/by-sa/4.0/ "License: CC BY-SA 4.0")'
---

## What we will cover today

* Show practical steps for obtaining an R package implementation of a statistical method
  * Discuss key considerations for writing statistical software
  * Illustrate throughout with the `mmrm` package development example
* Short hands-on introduction to the `mmrm` package
  * Working together across companies and in open source
  
# Key considerations for writing statistical R packages

## Why does this matter in Pharma?

"*The credibility* of the numerical results of the analysis *depends on the quality and validity of the methods and software* (both internally and externally written) used both for data management [...] and also *for processing the data statistically*. [...] The *computer software* used for data management and statistical analysis *should be reliable*, and documentation of *appropriate software testing* procedures should be available."

[ICH Topic E 9: Statistical Principles for Clinical Trials, Section 5.8: Integrity of Data and Computer Software Validity]

## How can we achive this?

How can we implement statistical methods in R such that

- the software is reliable and 
- includes appropriate testing

to ensure 

- high quality and 
- validity 

and ultimately credibility of the statistical analysis results? 

## Take away lessons for writing statistical software

1. Choose the right methods and understand them.
1. Solve the core implementation problem with prototype code.
1. Spend enough time on planning the design of the R package.
1. Assume that your R package will be evolving for a long time.

# Choose the right methods and understand them

## Why is this important?

"*The credibility* of the numerical results of the analysis *depends on the quality and validity of the methods* and software ..."

* If we don't choose the right method, then the best software implementation of it won't help the credibility of the statistical analysis!
* Work together with methods experts (internal, external, ...)

## How can we understand the statistical method?

We need to understand the method before implementing it!

- It is not sufficient to just copy/paste code from methods experts
- Let the methods experts present a summary of the methods
- Read an overview paper about the methods
- Paraphrase and ask lots of clarifying questions
- Understand the details by reading the original paper describing the method

## Example: `mmrm`

- Understand the acronym: Mixed Model with Repeated Measures 
- Understand the method: It is *not* a mixed model, just a general linear model
  - Read an overview paper
- Understand the problem: In R we did not get the correct adjusted degrees of freedom
  - Try out existing R packages and compare results with proprietary software
  - Read paper describing the adjusted degrees of freedom
  
## Example: `mmrm` (fast forward)

- Initial implementation with `lme4` workaround (see previous R/Pharma presentation)
- Works only quite ok for small data sets with few time points
- Does not converge and takes hours on large data sets with many time points
- Therefore needed to look for another solution

# Solve the core implementation problem with prototype code

## What is prototype code?

- Can come in different forms, but 
  - is not an R package yet, 
  - not documented with `roxygen2`, 
  - not unit tested
- It works usually quite well to have an `Rmd` or `qmd` document to combine thoughts and code
- Typically an R script from a methods expert that implements the method can be the start for a prototype

## When have you solved the core implementation problem?

- You have R code that allows you to (half-manually) calculate the results with the chosen methods
- Different methods options have been elicited from experts, considered and could be used
- You have evaluated different solution paths (e.g. using package A or package B as a backbone)
- You feel that you have solved the hardest part of the problem - everything else is clear how to do it
- (If possible) You have compared the numerical results from your R code with other software,
  and they match up to numerical accuracy (e.g. relative difference of 0.001)

## Example: `mmrm` - try to use existing packages

- The hardest part: adjusted degrees of freedom calculation
- Tried different solutions with existing R packages:
  - using package `nlme` with `emmeans` (results are too different, calculation is too approximate)
  - using package `lme4` with `lmerTest` (fails on large data sets with many time points)
  - using package `glmmTMB` (does not have adjusted degrees of freedom)ÃŸ

## Example: `mmrm` - try to extend existing package

Tried to extend `glmmTMB` to calculate Satterthwaite adjusted degrees of freedom:

- Got in touch with `glmmTMB` authors and Ben Bolker provided great help and assistance
- Unfortunately it did not work out (results were very far off for unstructed covariance)
- Understand that `glmmTMB` always uses a random effects model representation which is not what we want

## Example: `mmrm` - try to make a custom implementation

Idea was then to use the Template Model Builder (`TMB`) library directly:

- As the name suggests, `TMB` is also used by `glmmTMB` as the backend
- We can code with `C++` the likelihood of the model we exactly need (general linear model without random effects)
- The magic: `TMB` allows to automatically differentiate the (log) likelihood (i.e. by compiling the `C++` code)
- The gradient (and Hessian) can then be used from the R side to find the (restricted) maximum likelihood estimates
- Within a long weekend, got a working prototype that was fast and matched proprietary software results nicely

# Spend enough time on planning the design of the R-package

## Why not jump into writing functions right away?

- Need to see the "big picture" first to know how each piece should look like
  - Including definition of the scope of the package - what should be included vs. not
  - Important to discuss "big picture" plan with experts and users regarding workflow
- When writing a function you should do it together with documentation and unit tests
  - If you just start somewhere, chances are very high that you will need to change it later

## How to plan the design of the R-package?

1. Start with blank sheet of paper to draw flow diagram 
   - What parts (functions and classes) can represent the problem most naturally?
1. Draft a bit more details in doc 
   - Which arguments for functions, which slots for classes? Names?
1. Go into `Rmd` design document and draft prototypes for functions and classes
1. Break down design into separate issues (tasks) to implement
   - Make notes of dependencies and resulting order of implementation
  
## Example: `mmrm`

- Have a single `Rmd` as initial design document including prototypes

```{r}
#| echo: false
#| warning: false
#| class-output: "sourceCode r"

cat(readLines("resources/_design_fit.Rmd"), sep = "\n")
```

## Example: `mmrm` (cont'd)

- Have separate issues and corresponding pull requests implementing functions

![](resources/issues.png)

# Assume your R-package is evolving for a long time

## Why should we document the methods?

- It is important to add method documentation in your package, typically as a vignette
- This provides the "glue" between the original methods paper and your implementation in code
  - different mathematical symbols compared to original paper but that match the code variable names
  - specific details on the algorithm
- Users benefit from this method documentation a lot because they can understand what is going on in your package
- Developers will depend on the method documentation when adding new method features and to understand the code

## Why do we need tests?

- It is 100% guaranteed that users will have new feature requests after the first version of the R package has been released 
  - This is a good sign - the package is being used and your users have good ideas!
- It is also very likely that some of the packages you depend on will change - but you want to be sure your package still works 
  - incl. integration tests, making sure numerical results are still correct
- So you will need to change the code ...
  - ... but you can only do that comfortably if you know that the package still works afterwards
  - If the tests pass you know it still works!

## How can I make the package extensible?

- "Extensible" = others can extend it without changing package code
- You want to make your package extensible so that you or other developers can easily extend it
- Prefer to combine multiple functions in pipelines (similar as typically done in tidyverse)
- Prefer object oriented package designs because it will help a lot the extensibility 
- Generally avoid functions with many arguments or longer than 50 lines of code

## Example: `mmrm` - method documentation

- Started with handwritten notes of the algorithm implementation for the prototype
- Translated that into vignette
- Has been updated many times already when algorithm was updated
- Meanwhile have in total 12 different vignettes on different aspects

## {background-iframe="https://openpharma.github.io/mmrm/latest-tag/articles/algorithm.html" background-interactive="true"}

## Example: `mmrm` - tests

- Add tests, code documentation and method documentation during each pull request for each function
  - Add integration tests comparing numerical results with numbers sourced from proprietary software
  - Some tests can take longer, if run time becomes an issue can skip them on CRAN
- Turned out tests were super important because minor `C++` changes could break results on different operating system

## Example: `mmrm` - extensibility

- This is a typical "model fitting" package and therefore we use the S3 class system
  - Over time can add interfaces to other modeling packages (more later)
  
# Introduction to the `mmrm` package

## Installation

- CRAN as usual: `install.packages("mmrm")` 
  - Lags a bit behind at the moment (due to CRAN manual review bottleneck)
- GitHub as usual: `remotes::install_github("openpharma/mmrm")`
  - But needs `C++` toolchain and can take quite a while to compile
- R-Universe: [https://openpharma.r-universe.dev/mmrm](https://openpharma.r-universe.dev/mmrm) and download the binary package and install afterwards
  - Somehow the `install.packages()` path from R does not find the binaries

## Features of `mmrm` (>= 0.3)

- Linear model for dependent observations within independent subjects
- Covariance structures for the dependent observations:
  - Unstructured, Toeplitz, AR1, compound symmetry, ante-dependence, spatial exponential
  - Allows group specific covariance estimates and weights
- REML or ML estimation, using multiple optimizers if needed
- Robust sandwich estimator for covariance
- Degrees of freedom adjustments: 
  Satterthwaite, Kenward-Roger, Kenward-Roger-Linear, Between-Within, Residual

## Ecosystem integration

- `emmeans` interface for least square means
- `tidymodels` for easy model fitting:
  - Dedicated `parsnip` engine for linear regression
  - Integration with `recipes`
- NEST family for clinical trial reporting:
  - `tern.mmrm` to easily generate common tables and plot (coming to CRAN soon)
  - `teal.modules.clinical` to easily spin up Shiny app based on the `teal` framework (coming to CRAN soon)
- Provided by third party packages (remember the extensibility discussion):
  - interfaces to `insight`, `parameters`

## Unit and integration testing

- Unit tests can be found in the GitHub repository under [./tests](https://github.com/openpharma/mmrm/tree/main/tests/testthat).
- Uses the `testthat` framework with `covr` to communicate the testing coverage.
  - Coverage above 95%
  - Also include tests for key `C++` functions
- The integration tests in `mmrm` are set to a standard tolerance of $10^{-3}$ when compared to other software outputs
  - Comparison with SAS results (`PROC MIXED`)
  - Comparison with relevant R packages

## Benchmarking with other R packages

- Compared `mmrm::mmrm` with `nlme::gls`, `lme4::lmer`, `glmmTMB::glmmTMB`
- Highlights:
  - `mmrm` has faster convergence time 
    - Using `FEV` dataset as an example, `mmrm` took ~50 ms, while `lmer` ~200 ms, `gls` and `glmmTMB` >600 ms
  - `mmrm` and `gls` estimates have smaller differences from SAS `PROC GLIMMIX` estimates
  - `mmrm` and `gls` are more resilient to missingness
- Detailed results at the online [comparison vignette](https://openpharma.github.io/mmrm/main/articles/mmrm_review_methods.html)

## Impact of `mmrm`

- CRAN downloads: around 100 per day in Oct 2023
  - [https://cran.r-project.org/web/packages/mmrm/](https://cran.r-project.org/web/packages/mmrm/) 
  - new CRAN release v0.3 coming any day now!
- GitHub repository: 73 stars as of 17 Oct 2023
  - [https://github.com/openpharma/mmrm](https://github.com/openpharma/mmrm) 
- Part of CRAN clinical trials task view

## Outlook

- `mmrm` is now relatively complete for mostly needed features
- We still have a few major ideas on our backlog:
  - Type II and Type III ANOVA tests
  - Evaluate adding (simple) random effects
- Please let us know what is missing in `mmrm` for you!

# Hands-On Demo Time!

## Demo Instructions

::: columns
::: {.column width="50%"}
* Head to [posit.cloud]()
* Open the "`{mmrm}` Workbench" Space
* Open the `mmrm-introduction.Rmd`
:::

::: {.column width="50%"}
![](resources/mmrm-workbench-posit-cloud.png)
:::
:::

# Open source development across companies

## Introducing `openstatsware`

::: columns
::: {.column width="70%"}
Founded last year: 

- When: 19 August 2022 - just celebrated our 1 year birthday!
- Where: American Statistical Association (ASA) Biopharmaceutical Section (BIOP)
- Initially: 11 statisticians from 7 pharma companies developing statistical software
- New name: `openstatsware`
:::

::: {.column width="30%"}
![](https://github.com/RConsortium/asa-biop-swe-wg/raw/main/sticker/sticker-new-1200.png){height="300"}
:::
:::

## `mmrm` was our first workstream

- Why is the MMRM topic important?
  - MMRM is a popular analysis method for longitudinal continuous outcomes in randomized clinical trials
  - Also used as backbone for more recent methods such as multiple imputation
- See also our second workstream that produced [`brms.mmrm`](https://openpharma.github.io/brms.mmrm/)
  - Bayesian inference in MMRM, based on `brms` (as Stan frontend for HMC sampling)

## Human success factors

- Mutual interest and mutual trust
- Prerequisite is getting to know each other
  - Although mostly just online, biweekly calls help a lot with this
- Reciprocity mindset
  - "Reciprocity means that in response to friendly actions, people are frequently much nicer and much more cooperative than predicted by the self-interest model"
  - Personal experience: If you first give away something, more will come back to you.

## Be inclusive in the development

- Important to go public as soon as possible 
  - We did not wait for `mmrm` to be finished before initial open sourcing
  - Many developers contributed over time
- Building software together works better than alone
  - Different perspectives in discussions and code review help to optimize the user interface and thus experience
  - Be generous with authorship

## Practical daily development process

Let's take a look at what it looks like in action. 

> **Case Study:** `simulate()` feature
> We'll follow the addition of a major `v0.3.0` feature, tracking its progress 
> our team's workflow.

## 1. Open Communication During Design

::: columns
::: {.column width="30%"}
Features start with a discussion to make sure we're aligned with the need and 
approach.
:::

::: {.column width="70%"}
![](resources/dev-feature-request.png)
:::
:::

## 2. Live Discussion at Bi-weekly Call

Bi-weekly calls allow an opportunity to discuss the details of an approach 
in more depth. 

::: columns
::: {.column width="50%"}
### Live Chat

* Great for initial planning,
* and aligning on final details
:::

::: {.column width="50%"}
### Async Discussion Threads

* Great for mocking up proposals
* Visual/code-based content like user-interface proposals
* Technical details
:::
:::

## 3. Initial Implementation

After deciding on the path forward, feature is often assigned out to an
an individual and an initial pass is put forth through a PR.

::: columns
::: {.column width="30%"}
The initial work is put forth with confidence about _what_ needs to be done, but
perhaps not _how_ it needs to be done
:::

::: {.column width="70%"}
![](resources/dev-pr.png)
:::
:::

## 4. Review

::: columns
::: {.column width="30%"}
Many small decisions recieve feedback until all concerns have been addressed.
:::

::: {.column width="70%"}
![](resources/dev-review.png)
:::
:::

## 5. Merge ðŸŽ‰

::: columns
::: {.column width="30%"}
When all concerns have been addressed, new code is introduced.
:::

::: {.column width="70%"}
![](resources/dev-merge.png)
:::
:::

## Workflow Recap

1. Discussion preceeds work on new features
1. Alignment is emphasized at Bi-weekly calls before embarking on development
1. Volunteer-based implmentation, depending on availability, expertise and interest
1. Calls allow for intermitent feedback throughout - especially important for larger features
1. PR submitted, with appropiately intensive review
1. When concerns are addressed, feature is incorporated


